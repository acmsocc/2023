stoica:
    name: 'Ion Stoica (UC Berkeley)'
    # title: 'Scalable Input Data Processing for Resource-Efficient Machine Learning' 
    # abstract: 'Data is the lifeblood of machine learning. Yet, our system infrastructure for managing and preprocessing training data in ML jobs lags behind the vast advancements in hardware accelerators, software frameworks, and algorithms that optimize model training computations. The input data pipeline in an ML job is responsible for extracting data from storage, transforming data on-the-fly, and loading data to a training node (typically a GPU or TPU). As hardware accelerators continues to provide more FLOPS, feeding data at a sufficient rate to saturate accelerators is increasingly challenging. The high cost of accelerators compared to their CPU hosts makes it particularly important to ensure that they operate at high utilization. Hence, the input pipeline is critical to the end-to-end throughput and cost of ML jobs. In this talk, we will discuss the characteristics of real ML input pipelines from production workloads which have led to the trend of disaggregating input data processing from model training. I will present recent open-source systems such as tf.data service and Cachew, which leverage a disaggregated system architecture to scale-out and optimize data processing within and across jobs. These systems alleviate input bottlenecks and dramatically improve the training time and cost of ML jobs.' 
    # bio1: "Ana Klimovic is an Assistant Professor in the Systems Group of the Computer Science Department at ETH Zurich. Her research interests span operating systems, computer architecture, and their intersection with machine learning. Ana's work focuses on computer system design for large-scale applications such as cloud computing services, data analytics, and machine learning. Before joining ETH in August 2020, Ana was a Research Scientist at Google Brain and completed her Ph.D. in Electrical Engineering at Stanford University."

kozyrakis:
    name: 'Christos Kozyrakis (Stanford)'
    # title: 'Declaring the Era of Programmable Clouds'
    # abstract: "For a decade or more, the main business of public cloud vendors has been to replace traditional enterprise computing systems with similar hosted services. This brought once-in-a-lifetime disruption to the business landscape. In comparison, the ambitions for innovation in cloud software engineering have been relatively modest. Given its revolutionary scale and potential, the cloudâ€™s real impact on software innovation is likely yet to come. 
    # bio1: "Joseph M. Hellerstein's work focuses on data-centric systems and the way they drive computing. He is the Jim Gray Professor of Computer Science at UC Berkeley, an ACM Fellow, an Alfred P. Sloan Research Fellow and the recipient of four \"Test of Time\" awards for his research. MIT's Technology Review magazine included his work on cloud programming in their TR10 list of the 10 technologies \"most likely to change our world\".

    
brooker:
    name: 'Marc Brooker (AWS)'
    # title: 'Systems for ML and ML for Systems:  A Virtuous Cycle'
    # abstract: "This talk is about the virtuous interplay between machine learning (ML) and systems. I will show examples of how systems optimized for ML computation can be used to train more accurate and capable ML models and how these ML models can be used to improve upon the ad-hoc heuristics used in system design and management. These improved systems can then be used to train better ML models. The latest trend in ML is the development of Foundation models. Foundation models are large pretrained models that have obtained state-of-the-art quality in natural language processing, vision, speech, and other areas. These models are challenging to train and serve because they are characterized by billions of parameters, irregular data access (sparsity) and irregular control flow. I will explain how Reconfigurable Dataflow Accelerators (RDAs) can be designed to accelerate foundation models with these characteristics. SambaNova Systems is using RDA technology to achieve record-setting performance on foundation models.  I will describe how the RDAs can also be used to build Taurus, an intelligent network data plane that enables ML models to be used to manage computer networks at full line-rate bandwidths. In particular, a Taurus prototype detects two orders of magnitude more events in a security application than a state-of-the-art system based on conventional network technology."
    # bio1: "Kunle Olukotun is the Cadence Design Professor of Electrical Engineering and Computer Science at Stanford University. Olukotun is a pioneer in multicore processor design and the leader of the Stanford Hydra chip multiprocessor (CMP) research project. He founded Afara Websystems to develop high-throughput, low-power multicore processors for server systems. The Afara multi-core processor, called Niagara, was acquired by Sun Microsystems and now powers Oracle's SPARC-based servers. In 2017, Olukotun co-founded SambaNova Systems, a Machine Learning and Artificial Intelligence company, and continues to lead as their Chief Technologist. Olukotun is the Director of the Pervasive Parallel Lab and a member of the Data Analytics tor What's Next (DAWN) Lab, developing infrastructure for usable machine learning. He is a member of the National Academy of Engineering, an ACM Fellow, and an IEEE Fellow for contributions to multiprocessors on a chip design and the commercialization of this technology. He also received the Harry H. Goode Memorial Award.  Olukotun received his Ph.D. in Computer Engineering from The University of Michigan."

tan:
    name: 'Wang-Chiew Tan (Facebook AI)'
    title: 'Querying Unstructured and Structured Data with Large Language Models'
    abstract: "Recently, Large Language Models (LLMs) have emerged as a powerful tool for accessing parametric knowledge, but the potential of effectively tapping into the vast expanse of external or private data remains largely unexplored. This talk presents an open-source question-answering system for seamlessly integrating model parameters with knowledge from external data sources to enhance its predictive capabilities.  Our larger vision transcends question answering. We envision a personal insight assistant, adept at sifting through one's past data to offer invaluable insights to help one make informed decisions and plan with foresight."
    bio1: "Wang-Chiew is a research scientist at Meta AI. Before she was the Head of Research at Megagon Labs, where she led the research efforts on building advanced technologies to enhance search by experience. Prior to joining Megagon Labs, she was a Professor of Computer Science at the University of California, Santa Cruz. She also spent two years at IBM Research - Almaden. She co-authored best papers, she is a co-recipient of the 2014 ACM PODS Alberto O. Mendelzon Test-of-Time Award, the 2018 ICDT Test-of-Time Award, and the 2020 Alonzo Church Award. She received the 2019 VLDB Women in Database Research Award, the 2021 National University of Singapore Outstanding Computing Alumni Award, and she is a Fellow of the ACM."